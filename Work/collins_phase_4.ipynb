{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP twitter Sentiments about Apple and Google Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "This project aims to develop a Natural Language Processing (NLP) model to analyze sentiment in Tweets related to Apple and Google products. By classifying the sentiment of these Tweets as positive, negative, or neutral, the model will provide valuable insights into public perception, aiding businesses in marketing strategies and product development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business and Data Understanding\n",
    "\n",
    "The dataset comprises Tweets labeled for sentiment (positive, negative, or neutral) towards various tech products, primarily from Apple and Google. This data is highly relevant for sentiment analysis, as it captures real-time user opinions and experiences with these products, offering valuable insights for marketing strategies and product development.\n",
    "Key Descriptive Statistics:\n",
    "\n",
    "    .   Total Tweets Analyzed: 9,000+\n",
    "    .   No Emotion Toward Brand/Product: 5,389 (approximately 60%)\n",
    "    .   Positive Emotion: 2,978 (approximately 33%)\n",
    "    .   Negative Emotion: 570 (approximately 6%)\n",
    "    .   Unclear Sentiment: 156 (approximately 2%)\n",
    "\n",
    "Sentiment Distribution:\n",
    "\n",
    "    Positive Sentiment: ~33%\n",
    "    Negative Sentiment: ~6%\n",
    "    Neutral Sentiment (No Emotion): ~60%\n",
    "    Unclear Sentiment: ~2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Objectives\n",
    "\n",
    "    1. Sentiment Classification: Develop a model that accurately classifies Tweets about Apple and Google products into three sentiment categories: positive, negative, and neutral.\n",
    "\n",
    "    2. Insights Generation: Provide actionable insights into consumer perceptions of Apple and Google products, helping to inform marketing strategies and product development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Preparation\n",
    "\n",
    "Data preparation steps included:\n",
    "\n",
    "    - Text Cleaning: Removal of URLs, mentions, and special characters to focus on the actual sentiment-laden content of the Tweets.\n",
    "    - Tokenization: Splitting Tweets into individual words (tokens) to facilitate analysis.\n",
    "    - Stopword Removal: Eliminating common words (like \"the,\" \"is,\" etc.) that do not contribute to sentiment analysis, allowing the model to focus on more meaningful words.\n",
    "\n",
    "Packages used:\n",
    "\n",
    "    Pandas: For data manipulation and cleaning.\n",
    "    NLTK (Natural Language Toolkit): For tokenization and stopword removal.\n",
    "    Scikit-learn: For model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import nltk\n",
    "import plotly.express as px\n",
    "import os  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n",
      "                                               tweet_text  \\\n",
      "count                                                9092   \n",
      "unique                                               9065   \n",
      "top     RT @mention Marissa Mayer: Google Will Connect...   \n",
      "freq                                                    5   \n",
      "\n",
      "       emotion_in_tweet_is_directed_at  \\\n",
      "count                             3291   \n",
      "unique                               9   \n",
      "top                               iPad   \n",
      "freq                               946   \n",
      "\n",
      "       is_there_an_emotion_directed_at_a_brand_or_product  \n",
      "count                                                9093  \n",
      "unique                                                  4  \n",
      "top                    No emotion toward brand or product  \n",
      "freq                                                 5389  \n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "features_df = pd.read_csv('data/Sentiments_analysis.csv', encoding='ISO-8859-1')  # Adjust encoding as needed\n",
    "\n",
    "# Display the structure and basic information of the dataset\n",
    "features_df.info()  # This will print the information directly\n",
    "\n",
    "# Optionally, if you want to store some summary information\n",
    "features_info = features_df.describe()  # This provides statistical details of numerical columns\n",
    "print(features_info)  # Print the summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values in 'tweet_text' by filling them with an empty string\n",
    "features_df['tweet_text'] = features_df['tweet_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Text Processing\n",
    "\n",
    "Text Preprocessing is an important step for natural language processing (NLP). This process will bring our reviews into a form that is predictable and analyzable for our neural network.\n",
    "\n",
    "For a future verification of our model we will build a pipeline that contains all of the steps of our text preprocessing. So here we will build functions to lowercase the text and remove HTML, remove accented characters, extended contractions, remove special characters, lemmatization, removing stop words, checking if is an english word and removing digits and duplicates white spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Lowercase and Remove HTML\n",
    "For lowercase and remove the html as in almost all of the nexts functions we will use the Regular Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def re_tags(tweet_text): #define remove tag funtion\n",
    "    return [TAG_RE.sub('', str(word)).lower() for word in tweet_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Removing Accented Characters\n",
    "\n",
    "For remove the accented characters we will use the module and unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_accented_char(tweet_text):\n",
    "   \n",
    "    return [unidecode.unidecode(word.encode().decode('utf-8')) for word in tweet_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Extended Contractions\n",
    "To extend the contractions. First we will replace possible mistakes to the right contraction, then we will use the re.sub functions to extend then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_contractions(tweet_text):\n",
    "    result=[]\n",
    "    for word in tweet_text:\n",
    "        # replace contracting withoutsignal\n",
    "        word = word.replace(\"wont\",\"won't\")\n",
    "        word = word.replace(\"cant\",\"can't\")\n",
    "        word = word.replace(\"its\",\"it's\")\n",
    "        word = word.replace(\"youre\",\"you're\")\n",
    "        word = word.replace(\"hes\",\"he's\")\n",
    "        word = word.replace(\"shes\",\"she's\")\n",
    "        word = word.replace(\"its\",\"it's\")\n",
    "        word = word.replace(\"weare\",\"we're\")\n",
    "        word = word.replace(\"theyre\",\"they're\")\n",
    "\n",
    "        # specific\n",
    "        word = re.sub(r\"won\\'t\", \"will not\", str(word))\n",
    "        word = re.sub(r\"can\\'t\", \"can not\", str(word))\n",
    "\n",
    "        # general\n",
    "        word = re.sub(r\"n\\'t\", \" not\", str(word))\n",
    "        word = re.sub(r\"\\'re\", \" are\", str(word))\n",
    "        word = re.sub(r\"\\'s\", \" is\", str(word))\n",
    "        word = re.sub(r\"\\'d\", \" would\", str(word))\n",
    "        word = re.sub(r\"\\'ll\", \" will\", str(word))\n",
    "        word = re.sub(r\"\\'t\", \" not\", str(word))\n",
    "        word = re.sub(r\"\\'ve\", \" have\", str(word))\n",
    "        word = re.sub(r\"\\'m\", \" am\", str(word))\n",
    "        result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Removing Special Characters\n",
    "To remove the special characters we will use the re.sub functions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_special_chars(tweet_text):\n",
    "    return [re.sub(\"[^a-zA-Z0-9]\",\" \",word) for word in tweet_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Lemmatization\n",
    "Lemmatization is the process of converting a word to its base form. To do the Lemmatization we will use the NLTK libraries. In order to lemmatize we will create an instance of the WordNetLemmatizer() and call the lemmatize() function on each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tweet_text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatizer_sentence = []  \n",
    "    tokenizer=nltk.tokenize.WhitespaceTokenizer()\n",
    "    for word in tokenizer.tokenize(tweet_text):\n",
    "        lemmatizer_sentence.append(wnl.lemmatize(word,'v'))\n",
    "        lemmatizer_sentence.append(\" \")\n",
    "    \n",
    "    return(\"\".join(lemmatizer_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Removing Stop Words\n",
    "To remove stop words from the reviews, we will tokenize the sentence and then remove the word if it exists in the list of stop words provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_text(tweet_text):\n",
    "    stop = stopwords.words('english')\n",
    "    sentence_without = []\n",
    "    tokenizer=nltk.tokenize.WhitespaceTokenizer()\n",
    "    for word in tokenizer.tokenize(tweet_text):\n",
    "        if word not in stop:\n",
    "            sentence_without.append(word)\n",
    "            sentence_without.append(\" \")\n",
    "            \n",
    "    return(\"\".join(sentence_without))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 - Checking If Is An English Word\n",
    "To check if the words are in the English dictionary we will use NLTK words corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=set(nltk.corpus.words.words())\n",
    "\n",
    "def word_check(tweet_text):\n",
    "    result=[]\n",
    "    for word in tweet_text:\n",
    "        if word.lower() in words:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 - Removing Digits And Duplicates White Spaces\n",
    "Last but not least we will use the re.sub again to remove the duplicates white spaces and the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_whitespaces(tweet_text): \n",
    "    result=[]\n",
    "    for word in tweet_text:\n",
    "        word=(re.sub(r'\\d','dig',str(word))) #remove numbers \n",
    "        word = (re.sub(r'\\s+',' ', str(word))) #remove duplicates white spacces\n",
    "        result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing blank comments\n",
    "features_df = features_df[features_df['tweet_text']!='']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Pipeline\n",
    "For the pipeline we are going to define 3 classes to apply all the text preprocessing previous functions on the tweet_texts. The first Class will apply all the functions created to prepare the text into a regular expression. The second Class will remove all the the stop words and the last Class will lemmatizer the remnants words of the previous processes processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for regular expressions application\n",
    "class ApplyRegex(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, regex_transformers):\n",
    "        self.regex_transformers = regex_transformers\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Applying all regex functions in the regex_transformers dictionary\n",
    "        for regex_name, regex_function in self.regex_transformers.items():\n",
    "            X = regex_function(X)\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordsRemoval(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, text_stopwords):\n",
    "        self.text_stopwords = text_stopwords\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return [self.text_stopwords(comment) for comment in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeProcess(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, Lemmatize):\n",
    "        self.Lemmatizer = Lemmatize\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return [self.Lemmatizer(comment) for comment in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define a dictionary for all the regular expressions functions and then define the text preprocessing pipeline with the classes above the defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining regex transformers to be applied\n",
    "regex_transformers = {\n",
    "    'remove_tags': re_tags,\n",
    "    'remove_accents': re_accented_char,\n",
    "    'decontracted': ex_contractions,\n",
    "    're_sc': re_special_chars,\n",
    "    'whitespaces': re_whitespaces\n",
    "}\n",
    "\n",
    "# Building a text prep pipeline\n",
    "text_prep_pipeline = Pipeline([\n",
    "    ('regex', ApplyRegex(regex_transformers)),\n",
    "    ('stopwords', StopWordsRemoval(stopwords_text)),\n",
    "    ('lemmatize', LemmatizeProcess(lemmatize_text)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the pipeline into the tweet_text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['tweet_text'] = text_prep_pipeline.fit_transform(features_df[features_df.columns[:1]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a good understanding of the Apple and Google Products tweet reviews we are going to plot the text in three different ways.\n",
    "\n",
    "\n",
    "    . First we will plot the distribution of the sentiments using pie graph express plot from plotly.\n",
    "    . The second plot is the famous Word Cloud graph from the wordcloud library.\n",
    "    . The last plot is a bar plot for the n-gram, which is a sequence of n words most common shown together. We will plot the 1 grams, 2 grams, 3 grams and 4 grams for each sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 1 - Sentiment Distribution\n",
    "To plot the distribution of the sentiments in our df we will use the main df to plot the total of each sentiment in a pie graph using the library plotly.express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming features_df is the DataFrame\n",
    "total = features_df['emotion_in_tweet_is_directed_at'].value_counts().reset_index()\n",
    "total.columns = ['Emotion', 'Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_counts = features_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "hovertemplate": "label=%{label}<br>value=%{value}<extra></extra>",
         "labels": [
          "No emotion toward brand or product",
          "Positive emotion",
          "Negative emotion",
          "I can't tell"
         ],
         "legendgroup": "",
         "name": "",
         "showlegend": true,
         "type": "pie",
         "values": [
          5388,
          2978,
          570,
          156
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Emotion Directed at Brands/Products"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig = px.pie(emotion_counts_df, values='Count', names='Emotion', title='Emotion Directed at Brands/Products')\n",
    "\n",
    "# If you prefer to use the Series directly\n",
    "fig = px.pie(values=emotion_counts.values, names=emotion_counts.index, title='Emotion Directed at Brands/Products')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pie chart shows a large percentage of emotions is not directed to any brand or product. There is a higher percentage of positive emotions than negative therefore we are going to balance the df in later actions before applying the neural networking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 - Words Cloud\n",
    "\n",
    "\n",
    "Now we are going to do the famous Word Cloud graph, an image composed of words used in the Apple and Google Products tweets, in which the size of each word indicates its frequency.\n",
    "\n",
    "To do that we are going to create 4 df. Positive, \"i can't tell\", Negative and \"no emotions towards brand or product\" with the words that compose the reviews of each sentiment and we are going to count how many times it repeat using values_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexing.py:1596: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "c:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "features_df = pd.read_csv('data/Sentiments_analysis.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Step 1: Filter for Apple and Google products\n",
    "apple_keywords = ['iPhone', 'iPad', 'Apple']\n",
    "google_keywords = ['Google']\n",
    "\n",
    "# Handle NaN values in 'tweet_text'\n",
    "features_df['tweet_text'] = features_df['tweet_text'].fillna('')\n",
    "\n",
    "# Filter tweets\n",
    "apple_tweets = features_df[features_df['tweet_text'].str.contains('|'.join(apple_keywords), na=False)]\n",
    "google_tweets = features_df[features_df['tweet_text'].str.contains('|'.join(google_keywords), na=False)]\n",
    "\n",
    "# Step 2: Encode sentiment\n",
    "sentiment_mapping = {\n",
    "    'Positive emotion': 1,\n",
    "    'Negative emotion': -1,\n",
    "    'No emotion toward brand or product': 0,\n",
    "    'I can\\'t tell': 2\n",
    "}\n",
    "\n",
    "# Map sentiments using .loc\n",
    "apple_tweets.loc[:, 'sentiment'] = apple_tweets['is_there_an_emotion_directed_at_a_brand_or_product'].map(sentiment_mapping)\n",
    "google_tweets.loc[:, 'sentiment'] = google_tweets['is_there_an_emotion_directed_at_a_brand_or_product'].map(sentiment_mapping)\n",
    "\n",
    "# Step 3: Aggregate sentiment counts\n",
    "apple_sentiment_counts = apple_tweets['sentiment'].value_counts().reindex([2, 1, 0, -1], fill_value=0)\n",
    "google_sentiment_counts = google_tweets['sentiment'].value_counts().reindex([2, 1, 0, -1], fill_value=0)\n",
    "\n",
    "# Step 4: Visualization of sentiment counts\n",
    "labels = ['I can\\'t tell', 'Positive', 'No emotion', 'Negative']\n",
    "apple_values = [apple_sentiment_counts[2], apple_sentiment_counts[1], apple_sentiment_counts[0], apple_sentiment_counts[-1]]\n",
    "google_values = [google_sentiment_counts[2], google_sentiment_counts[1], google_sentiment_counts[0], google_sentiment_counts[-1]]\n",
    "\n",
    "x = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x, apple_values, width=0.4, label='Apple', color='blue', align='center')\n",
    "plt.bar([p + 0.4 for p in x], google_values, width=0.4, label='Google', color='orange', align='center')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Analysis of Tweets for Apple and Google')\n",
    "plt.xticks([p + 0.2 for p in x], labels)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 - N_Gram Plot\n",
    "That is a very nice way to understand the most common topics per sentiment. To do that we will use the module CountVectorizer from sklearn feature_extraction text library. It might seem a little bit of a mess at first look but it's not. We builded a function that count the 1,2,3 and 4 grams per sentiment and another function to define the dfs per sentiment calling the first function.\n",
    "Then we will end up with results df that contain a list of 12 dfs.\n",
    "\n",
    "    Mono-gram - positive neutral and negative.\n",
    "    Bi-gram - for positive neutral and negative.\n",
    "    Tri-gram - for positive neutral and negative.\n",
    "    Tetra-gram - for positive neutral and negative.\n",
    "\n",
    "Then we will use subplot to plot the top 5 most comuns n-grams for each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(data, n=1, sentiment='Positive'):\n",
    "    \"\"\"Extract n-grams from the tweets of a specific sentiment.\"\"\"\n",
    "    # Filter by sentiment\n",
    "    filtered_data = data[data['emotion_in_tweet_is_directed_at'] == sentiment]\n",
    "    \n",
    "    # Initialize CountVectorizer for n-grams\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    ngrams = vectorizer.fit_transform(filtered_data['tweet_text'])\n",
    "    \n",
    "    # Get n-gram counts\n",
    "    ngram_counts = ngrams.toarray().sum(axis=0)\n",
    "    \n",
    "    # Create a DataFrame of n-grams and their counts\n",
    "    ngram_df = pd.DataFrame(ngram_counts, index=vectorizer.get_feature_names_out(), columns=['count'])\n",
    "    ngram_df = ngram_df.sort_values(by='count', ascending=False)\n",
    "    \n",
    "    return ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for sentiment: Positive emotion\n",
      "No n-grams found for 'Positive emotion' at 1-gram size.\n",
      "No data available for sentiment: Negative emotion\n",
      "No n-grams found for 'Negative emotion' at 1-gram size.\n",
      "No data available for sentiment: No emotion toward brand or product\n",
      "No n-grams found for 'No emotion toward brand or product' at 1-gram size.\n",
      "No data available for sentiment: I can't tell\n",
      "No n-grams found for 'I can't tell' at 1-gram size.\n",
      "No data available for sentiment: Positive emotion\n",
      "No n-grams found for 'Positive emotion' at 2-gram size.\n",
      "No data available for sentiment: Negative emotion\n",
      "No n-grams found for 'Negative emotion' at 2-gram size.\n",
      "No data available for sentiment: No emotion toward brand or product\n",
      "No n-grams found for 'No emotion toward brand or product' at 2-gram size.\n",
      "No data available for sentiment: I can't tell\n",
      "No n-grams found for 'I can't tell' at 2-gram size.\n",
      "No data available for sentiment: Positive emotion\n",
      "No n-grams found for 'Positive emotion' at 3-gram size.\n",
      "No data available for sentiment: Negative emotion\n",
      "No n-grams found for 'Negative emotion' at 3-gram size.\n",
      "No data available for sentiment: No emotion toward brand or product\n",
      "No n-grams found for 'No emotion toward brand or product' at 3-gram size.\n",
      "No data available for sentiment: I can't tell\n",
      "No n-grams found for 'I can't tell' at 3-gram size.\n",
      "No data available for sentiment: Positive emotion\n",
      "No n-grams found for 'Positive emotion' at 4-gram size.\n",
      "No data available for sentiment: Negative emotion\n",
      "No n-grams found for 'Negative emotion' at 4-gram size.\n",
      "No data available for sentiment: No emotion toward brand or product\n",
      "No n-grams found for 'No emotion toward brand or product' at 4-gram size.\n",
      "No data available for sentiment: I can't tell\n",
      "No n-grams found for 'I can't tell' at 4-gram size.\n"
     ]
    }
   ],
   "source": [
    "def get_ngrams(data, n, sentiment):\n",
    "    # Filter data for the specified sentiment\n",
    "    filtered_data = data[data['emotion_in_tweet_is_directed_at'] == sentiment]\n",
    "\n",
    "    # Check if the filtered data is empty or contains no valid tweet_text\n",
    "    if filtered_data.empty or filtered_data['tweet_text'].isnull().all():\n",
    "        print(f\"No data available for sentiment: {sentiment}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "\n",
    "    # Initialize CountVectorizer for n-grams\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    ngrams = vectorizer.fit_transform(filtered_data['tweet_text'])\n",
    "\n",
    "    # Get n-gram counts\n",
    "    ngram_counts = ngrams.sum(axis=0).A1\n",
    "    ngram_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame of n-grams and their counts\n",
    "    ngram_df = pd.DataFrame({'ngram': ngram_list, 'count': ngram_counts})\n",
    "    ngram_df = ngram_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "    return ngram_df\n",
    "\n",
    "# Main extraction of n-grams\n",
    "ngram_dfs = []\n",
    "sentiments = ['Positive emotion', 'Negative emotion', 'No emotion toward brand or product', \"I can't tell\"]\n",
    "n_values = [1, 2, 3, 4]  # for mono, bi, tri, tetra\n",
    "\n",
    "for n in n_values:\n",
    "    for sentiment in sentiments:\n",
    "        ngram_df = get_ngrams(features_df, n=n, sentiment=sentiment)\n",
    "\n",
    "        # Handle empty DataFrames\n",
    "        if not ngram_df.empty:\n",
    "            ngram_dfs.append(ngram_df)\n",
    "        else:\n",
    "            print(f\"No n-grams found for '{sentiment}' at {n}-gram size.\")\n",
    "\n",
    "# Now ngram_dfs contains all the n-gram DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for Positive at 1-gram\n",
      "No data for Positive at 2-gram\n",
      "No data for Positive at 3-gram\n",
      "No data for Positive at 4-gram\n",
      "No data for Neutral at 1-gram\n",
      "No data for Neutral at 2-gram\n",
      "No data for Neutral at 3-gram\n",
      "No data for Neutral at 4-gram\n",
      "No data for Negative at 1-gram\n",
      "No data for Negative at 2-gram\n",
      "No data for Negative at 3-gram\n",
      "No data for Negative at 4-gram\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Mono-grams",
          "x": 0.10375,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.9999999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Bi-grams",
          "x": 0.36124999999999996,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.9999999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Tri-grams",
          "x": 0.61875,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.9999999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Tetra-grams",
          "x": 0.87625,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.9999999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Positive",
          "textangle": 90,
          "x": 0.98,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.8833333333333333,
          "yanchor": "middle",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Neutral",
          "textangle": 90,
          "x": 0.98,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.49999999999999994,
          "yanchor": "middle",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Negative",
          "textangle": 90,
          "x": 0.98,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.11666666666666665,
          "yanchor": "middle",
          "yref": "paper"
         }
        ],
        "height": 900,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.2075
         ]
        },
        "xaxis10": {
         "anchor": "y10",
         "domain": [
          0.2575,
          0.46499999999999997
         ]
        },
        "xaxis11": {
         "anchor": "y11",
         "domain": [
          0.515,
          0.7225
         ]
        },
        "xaxis12": {
         "anchor": "y12",
         "domain": [
          0.7725,
          0.98
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.2575,
          0.46499999999999997
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.515,
          0.7225
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.7725,
          0.98
         ]
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          0.2075
         ]
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.2575,
          0.46499999999999997
         ]
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0.515,
          0.7225
         ]
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0.7725,
          0.98
         ]
        },
        "xaxis9": {
         "anchor": "y9",
         "domain": [
          0,
          0.2075
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.7666666666666666,
          0.9999999999999999
         ]
        },
        "yaxis10": {
         "anchor": "x10",
         "domain": [
          0,
          0.2333333333333333
         ]
        },
        "yaxis11": {
         "anchor": "x11",
         "domain": [
          0,
          0.2333333333333333
         ]
        },
        "yaxis12": {
         "anchor": "x12",
         "domain": [
          0,
          0.2333333333333333
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.7666666666666666,
          0.9999999999999999
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.7666666666666666,
          0.9999999999999999
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.7666666666666666,
          0.9999999999999999
         ]
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0.3833333333333333,
          0.6166666666666666
         ]
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0.3833333333333333,
          0.6166666666666666
         ]
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0.3833333333333333,
          0.6166666666666666
         ]
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0.3833333333333333,
          0.6166666666666666
         ]
        },
        "yaxis9": {
         "anchor": "x9",
         "domain": [
          0,
          0.2333333333333333
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the subplot figure\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=4,\n",
    "    vertical_spacing=0.15,\n",
    "    column_titles=['Mono-grams', 'Bi-grams', 'Tri-grams', 'Tetra-grams'],\n",
    "    row_titles=['Positive', 'Neutral', 'Negative']\n",
    ")\n",
    "\n",
    "# Define colors for the different sentiments\n",
    "colors = ['DeepSkyBlue', 'Lightgrey', 'Crimson']\n",
    "\n",
    "# Total n-gram types (1 to 4)\n",
    "n_gram_types = 4\n",
    "\n",
    "# Loop through sentiments and n-gram types\n",
    "sentiment_indices = {\n",
    "    'Positive': range(0, n_gram_types),\n",
    "    'Neutral': range(n_gram_types, 2 * n_gram_types),\n",
    "    'Negative': range(2 * n_gram_types, 3 * n_gram_types)\n",
    "}\n",
    "\n",
    "for row, (sentiment, indices) in enumerate(sentiment_indices.items(), start=1):\n",
    "    for col in range(n_gram_types):\n",
    "        idx = indices[col]\n",
    "        if idx < len(ngram_dfs):  # Check if index is within bounds\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=ngram_dfs[idx]['ngram'].head(5),\n",
    "                y=ngram_dfs[idx]['count'].head(5),\n",
    "                marker_color=colors[row - 1],\n",
    "            ), row=row, col=col + 1)\n",
    "        else:\n",
    "            print(f\"No data for {sentiment} at {col + 1}-gram\")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout({\"showlegend\": False}, height=900, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For positive reviews we usually see easy use, love shop amazon or great customer service. For neutral reviews some specific topics that maybe people are facing troubles and it should be improved like wish list, kindle book, amazon smile or something to bring back. Negative reviews usually see people talking about the latest updates and we see some error messages like \"something went wrong\" among more other things.\n",
    "\n",
    "It makes way more sense isn't it?\n",
    "\n",
    "When smaller is the n_gram shows more similarity between the sentiments and the opposite happens when we increase the number of grams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Modeling\n",
    "Before we build our deep learning model, we are going to go through a few steps. The first one is to Split the data frame into Train and Test. Second, we will vectorize and embed the reviews. In order to prevent overfitting we will balance the train set and also reduce the amount of features of it. Then we are going to create 3 functions of metrics that will be used into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Train Test Split\n",
    "Now, we will divide our data into training and test sets using sklearn.model_selection module train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.dropna(subset=['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_data = features_df.dropna(subset=['tweet_text', 'emotion_in_tweet_is_directed_at'])\n",
    "X = clean_data['tweet_text']\n",
    "y = clean_data['emotion_in_tweet_is_directed_at']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our models with the logistic regression being the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:34:16] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_tfidf)  # Make predictions for Logistic Regression\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "\n",
    "# Fit XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Evaluate the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8467374810318664\n",
      "Logistic Regression Confusion Matrix:\n",
      " [[  5   2   0   0   0   0   5   1   0]\n",
      " [  1   3   0   0   0   0   2   4   0]\n",
      " [  0   0 120   0   0   0  10   0   0]\n",
      " [  0   0   1  69   0  14   2   0   0]\n",
      " [  0   0   1   0   0   0   1   2   1]\n",
      " [  0   0   0  13   0  41   0   1   0]\n",
      " [  0   0  11   0   0   0 192   1   0]\n",
      " [  0   0   2   0   0   0   8  88   6]\n",
      " [  0   0   1   0   0   0   4   7  40]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                        Android       0.83      0.38      0.53        13\n",
      "                    Android App       0.60      0.30      0.40        10\n",
      "                          Apple       0.88      0.92      0.90       130\n",
      "                         Google       0.84      0.80      0.82        86\n",
      " Other Apple product or service       0.00      0.00      0.00         5\n",
      "Other Google product or service       0.75      0.75      0.75        55\n",
      "                           iPad       0.86      0.94      0.90       204\n",
      "             iPad or iPhone App       0.85      0.85      0.85       104\n",
      "                         iPhone       0.85      0.77      0.81        52\n",
      "\n",
      "                       accuracy                           0.85       659\n",
      "                      macro avg       0.72      0.63      0.66       659\n",
      "                   weighted avg       0.84      0.85      0.84       659\n",
      "\n",
      "Random Forest Accuracy: 0.8528072837632777\n",
      "Random Forest Confusion Matrix:\n",
      " [[ 11   0   0   0   0   0   2   0   0]\n",
      " [  2   5   0   0   0   0   0   3   0]\n",
      " [  0   0 121   0   1   0   8   0   0]\n",
      " [  1   0   2  66   0  17   0   0   0]\n",
      " [  0   0   1   0   2   0   1   0   1]\n",
      " [  0   0   0  12   0  43   0   0   0]\n",
      " [  0   0  19   0   0   0 180   2   3]\n",
      " [  0   1   2   0   0   1   7  88   5]\n",
      " [  1   0   0   0   0   0   0   5  46]]\n",
      "Random Forest Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                        Android       0.73      0.85      0.79        13\n",
      "                    Android App       0.83      0.50      0.62        10\n",
      "                          Apple       0.83      0.93      0.88       130\n",
      "                         Google       0.85      0.77      0.80        86\n",
      " Other Apple product or service       0.67      0.40      0.50         5\n",
      "Other Google product or service       0.70      0.78      0.74        55\n",
      "                           iPad       0.91      0.88      0.90       204\n",
      "             iPad or iPhone App       0.90      0.85      0.87       104\n",
      "                         iPhone       0.84      0.88      0.86        52\n",
      "\n",
      "                       accuracy                           0.85       659\n",
      "                      macro avg       0.81      0.76      0.77       659\n",
      "                   weighted avg       0.86      0.85      0.85       659\n",
      "\n",
      "XGBoost Accuracy: 0.842185128983308\n",
      "XGBoost Confusion Matrix:\n",
      " [[  9   1   0   0   0   0   1   2   0]\n",
      " [  4   5   0   0   0   0   0   1   0]\n",
      " [  0   0 109   0   1   0  20   0   0]\n",
      " [  1   0   2  65   0  18   0   0   0]\n",
      " [  0   0   1   0   2   0   1   0   1]\n",
      " [  0   0   0  15   0  40   0   0   0]\n",
      " [  0   0   8   0   0   0 191   2   3]\n",
      " [  1   5   0   0   0   1   3  88   6]\n",
      " [  1   0   0   0   1   0   0   4  46]]\n",
      "XGBoost Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                        Android       0.56      0.69      0.62        13\n",
      "                    Android App       0.45      0.50      0.48        10\n",
      "                          Apple       0.91      0.84      0.87       130\n",
      "                         Google       0.81      0.76      0.78        86\n",
      " Other Apple product or service       0.50      0.40      0.44         5\n",
      "Other Google product or service       0.68      0.73      0.70        55\n",
      "                           iPad       0.88      0.94      0.91       204\n",
      "             iPad or iPhone App       0.91      0.85      0.88       104\n",
      "                         iPhone       0.82      0.88      0.85        52\n",
      "\n",
      "                       accuracy                           0.84       659\n",
      "                      macro avg       0.73      0.73      0.73       659\n",
      "                   weighted avg       0.85      0.84      0.84       659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Random Forest Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping emotions to include 'neutral'\n",
    "features_df['emotion_in_tweet_is_directed_at'] = features_df['emotion_in_tweet_is_directed_at'].replace({\n",
    "    'neutral': 'No emotion toward brand or product',\n",
    "    'positive': 'Positive emotion',\n",
    "    'negative': 'Negative emotion',\n",
    "    'uncertain': \"Uncertain about emotion\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "[18:02:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:13] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:16] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:22] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:30] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:36] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:40] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:45] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:50] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:00] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:09] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:18] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:28] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:42] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:46] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:50] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:57] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:04] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:11] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:18] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:31] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:45] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:58] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:06:12] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:06:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:06:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:06:44] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:06:50] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:06:55] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:00] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:05] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:23] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:31] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:39] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:08:04] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:08:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:08:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:08:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:11] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:16] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:24] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:28] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:33] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:42] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:09:55] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:05] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:22] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:31] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:34] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:44] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:11:00] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:11:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:11:13] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:11:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:11:30] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:11:42] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:11:53] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:04] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:20] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:24] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:28] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:32] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:36] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:44] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:52] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:12:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:13:07] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:13:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:13:28] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:13:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:13:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:08] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:23] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:28] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:30] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:33] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:35] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:40] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:45] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:50] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:14:55] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:15:01] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:15:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:15:22] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:15:33] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:15:44] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:15:55] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:00] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:05] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:09] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:29] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:49] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:58] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:17:08] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:17:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:17:45] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:18:04] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:18:26] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:18:50] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:19:00] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:19:09] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:19:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:19:32] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:19:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:20:04] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:20:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:20:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:20:58] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:21:16] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:21:46] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:22:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:22:42] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:23:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:23:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Best XGBoost Parameters: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf_grid_search = GridSearchCV(rf, rf_param_grid, cv=5, scoring='accuracy')\n",
    "rf_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best Random Forest Parameters:\", rf_grid_search.best_params_)\n",
    "best_rf = rf_grid_search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "xgb_grid_search = GridSearchCV(xgb, xgb_param_grid, cv=5, scoring='accuracy')\n",
    "xgb_grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best XGBoost Parameters:\", xgb_grid_search.best_params_)\n",
    "best_xgb = xgb_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Accuracy: 0.8619119878603946\n",
      "Confusion Matrix:\n",
      " [[ 11   0   0   0   0   0   2   0   0]\n",
      " [  2   4   0   0   0   0   0   4   0]\n",
      " [  0   0 124   0   0   0   6   0   0]\n",
      " [  0   0   2  67   0  15   2   0   0]\n",
      " [  0   0   1   0   1   0   2   0   1]\n",
      " [  0   0   0  14   0  41   0   0   0]\n",
      " [  0   0  15   0   0   0 185   2   2]\n",
      " [  0   0   2   0   0   1   5  91   5]\n",
      " [  1   0   0   0   0   0   2   5  44]]\n",
      "Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                        Android       0.79      0.85      0.81        13\n",
      "                    Android App       1.00      0.40      0.57        10\n",
      "                          Apple       0.86      0.95      0.91       130\n",
      "                         Google       0.83      0.78      0.80        86\n",
      " Other Apple product or service       1.00      0.20      0.33         5\n",
      "Other Google product or service       0.72      0.75      0.73        55\n",
      "                           iPad       0.91      0.91      0.91       204\n",
      "             iPad or iPhone App       0.89      0.88      0.88       104\n",
      "                         iPhone       0.85      0.85      0.85        52\n",
      "\n",
      "                       accuracy                           0.86       659\n",
      "                      macro avg       0.87      0.73      0.76       659\n",
      "                   weighted avg       0.86      0.86      0.86       659\n",
      "\n",
      "Best XGBoost Accuracy: 0.8482549317147192\n",
      "Confusion Matrix:\n",
      " [[ 11   1   0   0   0   0   1   0   0]\n",
      " [  5   5   0   0   0   0   0   0   0]\n",
      " [  0   0 110   0   0   0  20   0   0]\n",
      " [  1   0   3  68   0  14   0   0   0]\n",
      " [  0   0   1   0   2   0   1   0   1]\n",
      " [  0   0   0  19   0  36   0   0   0]\n",
      " [  0   0   7   0   0   0 193   2   2]\n",
      " [  0   5   0   1   0   0   5  85   8]\n",
      " [  1   0   0   0   0   0   0   2  49]]\n",
      "Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                        Android       0.61      0.85      0.71        13\n",
      "                    Android App       0.45      0.50      0.48        10\n",
      "                          Apple       0.91      0.85      0.88       130\n",
      "                         Google       0.77      0.79      0.78        86\n",
      " Other Apple product or service       1.00      0.40      0.57         5\n",
      "Other Google product or service       0.72      0.65      0.69        55\n",
      "                           iPad       0.88      0.95      0.91       204\n",
      "             iPad or iPhone App       0.96      0.82      0.88       104\n",
      "                         iPhone       0.82      0.94      0.87        52\n",
      "\n",
      "                       accuracy                           0.85       659\n",
      "                      macro avg       0.79      0.75      0.75       659\n",
      "                   weighted avg       0.85      0.85      0.85       659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest\n",
    "y_pred_rf_best = best_rf.predict(X_test_tfidf)\n",
    "print(\"Best Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf_best))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf_best))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf_best))\n",
    "\n",
    "# Evaluate XGBoost\n",
    "y_pred_xgb_best = best_xgb.predict(X_test_tfidf)\n",
    "print(\"Best XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb_best))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb_best))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_xgb_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "After processing the data and applying machine learning models, including Logistic Regression, Random Forest, and XGBoost, we were able to:\n",
    "\n",
    "    Evaluate Model Performance: We assessed the models using accuracy, confusion matrices, and classification reports. This allowed us to determine how well each model performed in classifying the different sentiments.\n",
    "    Hyperparameter Tuning: We optimized model parameters to improve predictive performance. The tuned models offered better accuracy and insights into sentiment classification.\n",
    "    Key Findings\n",
    "\n",
    "    Class Distribution: The dataset showed a predominant number of tweets classified as \"No Emotion Toward Brand or Product,\" indicating that many consumers are indifferent to brand messages. The positive sentiment was the second most common, followed by negative and uncertain sentiments.\n",
    "\n",
    "    Model Performance:\n",
    "        The tuned Random Forest and XGBoost models generally outperformed Logistic Regression, particularly in classifying the more nuanced sentiments (positive, negative, and neutral).\n",
    "        Accuracy varied based on the model used, but all models provided valuable insights.\n",
    "\n",
    "    Challenges:\n",
    "        The \"I Can't Tell\" sentiment posed challenges due to its ambiguity, leading to potential misclassifications. Additionally, imbalanced class distribution may affect model performance, particularly for minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations\n",
    "\n",
    "    Focus on Neutral Sentiment:\n",
    "        Engagement Strategies: Develop strategies to engage users who express neutrality. Target these consumers with tailored content that can potentially shift their sentiment to positive.\n",
    "        Content Creation: Use insights from neutral sentiment tweets to create content that addresses common themes or questions, aiming to evoke a stronger emotional response.\n",
    "\n",
    "    Enhance Positive Sentiment:\n",
    "        Amplify Positive Feedback: Promote positive sentiments in marketing campaigns and leverage testimonials from satisfied customers to enhance brand image.\n",
    "        Loyalty Programs: Consider implementing loyalty programs that reward positive engagement, encouraging customers to share their positive experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the original DataFrame to drop rows with NaN values in either column\n",
    "clean_data = features_df.dropna(subset=['tweet_text', 'emotion_in_tweet_is_directed_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_data['tweet_text']\n",
    "y = clean_data['emotion_in_tweet_is_directed_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we vectorize the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:14:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_log_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-530389e2859c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Logistic Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlog_reg_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_log_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_reg_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression Confusion Matrix:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_log_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_log_reg' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# Random Forest\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Random Forest Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# XGBoost\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_df['tweet_text']\n",
    "y = features_df['emotion_in_tweet_is_directed_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to ensure alignment between features and target\n",
    "train_data = pd.DataFrame({'tweet_text': X_train, 'emotion': y_train})\n",
    "\n",
    "# Drop rows with NaNs in either column\n",
    "train_data = train_data.dropna()\n",
    "\n",
    "# Re-split the data\n",
    "X_train = train_data['tweet_text']\n",
    "y_train = train_data['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-ba4c18ac0aef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred_log_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1206\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   1209\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     )\n\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1162\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[0mestimator_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_estimator_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"object\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input contains NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;31m# We need only consider float arrays, hence can early return for all else.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1359281e4eb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Logistic Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_pred_log_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1206\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   1209\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     )\n\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1162\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[0mestimator_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_estimator_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"object\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input contains NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;31m# We need only consider float arrays, hence can early return for all else.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\\x80',\n",
       " '\\x81',\n",
       " '\\x84',\n",
       " '\\x89',\n",
       " '\\x8a',\n",
       " '\\x8b',\n",
       " '\\x8c',\n",
       " '\\x8d',\n",
       " '\\x8e',\n",
       " '\\x8f',\n",
       " '\\x93',\n",
       " '\\x95',\n",
       " '\\x9d',\n",
       " '\\xa0',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¤',\n",
       " '¥',\n",
       " '©',\n",
       " 'ª',\n",
       " '«',\n",
       " '¬',\n",
       " '±',\n",
       " '´',\n",
       " 'µ',\n",
       " '»',\n",
       " '¼',\n",
       " '¾',\n",
       " 'Á',\n",
       " 'Ä',\n",
       " 'Ç',\n",
       " 'È',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'Ì',\n",
       " 'Î',\n",
       " 'Ï',\n",
       " 'Ð',\n",
       " 'Ò',\n",
       " 'Ó',\n",
       " 'Ô',\n",
       " 'Ù',\n",
       " 'Û',\n",
       " 'Ü',\n",
       " 'Ý',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'ë',\n",
       " 'ö',\n",
       " '÷',\n",
       " 'ü'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocabulary = set(word for tweet in data for word in tweet)\n",
    "total_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 149 unique tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# we check the length of the list\n",
    "len(total_vocabulary)\n",
    "print('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-f7ed10da4590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Remove any leading/trailing whitespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = features_df['is_there_an_emotion_directed_at_a_brand_or_product'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature set X from tweet_text column\n",
    "X = features_df[\"tweet_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Embedding\n",
    "We need to convert text inputs into embedded vectors such that we can apply machine learning. In word embeddings, every word is represented as an n-dimensional dense vector. The words that are similar will have similar vectors. See more about embedding\n",
    "\n",
    "In order to embed our text, first we need to transform our reviews into vector representations. Here we will use Tokenizer module from keras.preprocessing.text to vectorize the text corpus and we will need the module pad_sequences from keras.preprocessing.sequence to ensure that all sequences in the list of words have the same length.\n",
    "\n",
    "After vectorising the text and ensuring that all sequences have the same length we will use the GloVe, Global Vectors For Word Representation to convert text inputs to their numeric counterparts. See more about GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7274, 200)\n",
      "X_test shape: (1819, 200)\n",
      "y_train shape: (7274,)\n",
      "y_test shape: (1819,)\n"
     ]
    }
   ],
   "source": [
    "# Define the number of words to consider (top 5000)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "# Fitting the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenizing the training and test data\n",
    "X_train_prep = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_prep = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Get vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Set the maximum length of features\n",
    "maxlen = 200\n",
    "\n",
    "# Ensuring all sequences have the same length\n",
    "X_train = pad_sequences(X_train_prep, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test_prep, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Optional: Check the shape of the prepared data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using numpy arrays and GloVe word embeddings to convert text inputs to their numeric counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe file not found. Please check the path.\n",
      "Embedding matrix shape: (9177, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "embeddings_dictionary = dict()\n",
    "#glove_file_path = 'data/glove.6B.100d.txt'  # Adjust the path to your GloVe file\n",
    "glove_file_path = 'full/path/to/glove.6B.100d.txt'\n",
    "\n",
    "if os.path.isfile(glove_file_path):\n",
    "    with open(glove_file_path, encoding=\"utf8\") as glove_file:\n",
    "        for line in glove_file:\n",
    "            records = line.split()\n",
    "            word = records[0]\n",
    "            vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "            embeddings_dictionary[word] = vector_dimensions\n",
    "else:\n",
    "    print(\"GloVe file not found. Please check the path.\")\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Optional: Check the shape of the embedding matrix\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - Balancing\n",
    "\n",
    "We notice that the data set is highly unbalanced, to be more precise in the results we will balance the dataset using the imblearn Random OverSampling technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No emotion toward brand or product' 'Positive emotion'\n",
      " 'Negative emotion' \"I can't tell\"]\n"
     ]
    }
   ],
   "source": [
    "unique_classes = pd.Series(y_train).unique()\n",
    "print(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No emotion toward brand or product: 4306\n",
      "Positive emotion: 2389\n",
      "Negative emotion: 455\n",
      "I can't tell: 124\n"
     ]
    }
   ],
   "source": [
    "emotion_counts = pd.Series(y_train).value_counts()\n",
    "for emotion in unique_classes:\n",
    "    print(f'{emotion}: {emotion_counts.get(emotion, 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative emotion: 455\n",
      "No emotion toward brand or product: 4306\n",
      "Positive emotion: 2389\n",
      "I can't tell: 124\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each sentiment in y_train\n",
    "emotion_counts = pd.Series(y_train).value_counts()\n",
    "\n",
    "# Print the counts for each emotion type\n",
    "print('Negative emotion:', emotion_counts.get('Negative emotion', 0))\n",
    "print('No emotion toward brand or product:', emotion_counts.get('No emotion toward brand or product', 0))\n",
    "print('Positive emotion:', emotion_counts.get('Positive emotion', 0))\n",
    "print('I can\\'t tell:', emotion_counts.get('I can\\'t tell', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocabulary = set(word for tweet in data for word in tweet)\n",
    "total_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0 MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomOverSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-4f8d3ac69b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Balancing the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mX_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomOverSampler' is not defined"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'KNeighbors': KNeighborsClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Balancing the training data\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Evaluate each model using cross-validation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    score = cross_val_score(model, X_resampled, y_resampled, cv=5)\n",
    "    results[name] = score.mean()\n",
    "\n",
    "# Display results\n",
    "for model_name, accuracy in results.items():\n",
    "    print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 - Featuring Selection\n",
    "\n",
    "In order to prevent overfitting we are going to reduce the features of the data frame. To do that we are going to use one of the simplest and most common ways to select relevant features for classification which is to calculate the F-Score for each feature.\n",
    "\n",
    "The F-Score is calculated using the variance between the features and the variance within each feature. A small F-score usually means that the feature is less important than a feature with a high F-score. We will calculate the F-Score of the features per sentiment using sklearn modules SelectKBest and f_classif to return the ANOVA F-value.\n",
    "\n",
    "I highly recommend see the Nils Schlüter Medium article for more ways to prevent overfitting in Deep Learning Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [] \n",
    "for label in range(0,3):\n",
    "    selector = SelectKBest(f_classif, k='all')\n",
    "    selector.fit(X_ov, pd.DataFrame(y_ov)[label])\n",
    "    selected_features.append(list(selector.scores_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Negative emotion',pd.DataFrame(y_train).sum()[0])\n",
    "print('No emotion toward brand or product',pd.DataFrame(y_train).sum()[1])\n",
    "print('Positive emotion',pd.DataFrame(y_train).sum()[2])\n",
    "print('I cant tell',pd.DataFrame(y_train).sum()[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/judge_tweet_product_company.csv', encoding='ISO-8859-1', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv('data/judge_tweet_product_company.csv', encoding='ISO-8859-1', dtype={'id': np.int16})\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Features\n",
    "Distributions of meta features in classes and datasets can be helpful to identify disaster tweets. The meta features used for the analysis are;\n",
    "\n",
    "    word_count number of words in text\n",
    "    unique_word_count number of unique words in text\n",
    "    stop_word_count number of stop words in text\n",
    "    url_count number of urls in text\n",
    "    mean_word_length average character count in words\n",
    "    char_count number of characters in text\n",
    "    punctuation_count number of punctuations in text\n",
    "    hashtag_count number of hashtags (#) in text\n",
    "    mention_count number of mentions (@) in text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Step 1: Filter for Apple and Google products\n",
    "apple_keywords = ['iPhone', 'iPad']\n",
    "google_keywords = ['Google']\n",
    "\n",
    "# Handle NaN values in 'tweet_text'\n",
    "features_df['tweet_text'] = features_df['tweet_text'].fillna('')\n",
    "\n",
    "# Filter tweets\n",
    "apple_tweets = features_df[features_df['tweet_text'].str.contains('|'.join(apple_keywords), na=False)]\n",
    "google_tweets = features_df[features_df['tweet_text'].str.contains('|'.join(google_keywords), na=False)]\n",
    "\n",
    "# Step 2: Encode sentiment\n",
    "sentiment_mapping = {\n",
    "    'Positive emotion': 1,\n",
    "    'Negative emotion': -1,\n",
    "    'no emotions towards brand or product': 0,\n",
    "    'I can\\'t tell': 2\n",
    "}\n",
    "\n",
    "apple_tweets['sentiment'] = apple_tweets['emotion_in_tweet_is_directed_at'].map(sentiment_mapping)\n",
    "google_tweets['sentiment'] = google_tweets['emotion_in_tweet_is_directed_at'].map(sentiment_mapping)\n",
    "\n",
    "# Step 3: Aggregate sentiment counts\n",
    "apple_sentiment_counts = apple_tweets['sentiment'].value_counts().reindex([2, 1, 0, -1], fill_value=0)\n",
    "google_sentiment_counts = google_tweets['sentiment'].value_counts().reindex([2, 1, 0, -1], fill_value=0)\n",
    "\n",
    "# Step 4: Visualization of sentiment counts\n",
    "labels = ['I can\\'t tell', 'Positive', 'No emotion', 'Negative']\n",
    "apple_values = [apple_sentiment_counts[2], apple_sentiment_counts[1], apple_sentiment_counts[0], apple_sentiment_counts[-1]]\n",
    "google_values = [google_sentiment_counts[2], google_sentiment_counts[1], google_sentiment_counts[0], google_sentiment_counts[-1]]\n",
    "\n",
    "x = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x, apple_values, width=0.4, label='Apple', color='blue', align='center')\n",
    "plt.bar([p + 0.4 for p in x], google_values, width=0.4, label='Google', color='green', align='center')\n",
    "\n",
    "plt.xticks([p + 0.2 for p in x], labels)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Sentiment Analysis of Apple and Google Products')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Generate word clouds based on sentiment\n",
    "def generate_wordcloud(comments, title):\n",
    "    if comments:\n",
    "        words = ' '.join(comments).split(' ')\n",
    "        word_freq = pd.Series(words).value_counts()\n",
    "        if not word_freq.empty:\n",
    "            wc = WordCloud(width=1280, height=720, collocations=False, random_state=42,\n",
    "                           background_color='white', max_words=50).generate_from_frequencies(word_freq)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.title(title, size=25)\n",
    "            plt.imshow(wc, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No words available for {title}.\")\n",
    "    else:\n",
    "        print(f\"No comments available for {title}.\")\n",
    "\n",
    "# Prepare comments for word clouds\n",
    "pos_comments = list(apple_tweets[apple_tweets['sentiment'] == 1]['tweet_text']) + \\\n",
    "               list(google_tweets[google_tweets['sentiment'] == 1]['tweet_text'])\n",
    "neu_comments = list(apple_tweets[apple_tweets['sentiment'] == 0]['tweet_text']) + \\\n",
    "               list(google_tweets[google_tweets['sentiment'] == 0]['tweet_text'])\n",
    "neg_comments = list(apple_tweets[apple_tweets['sentiment'] == -1]['tweet_text']) + \\\n",
    "               list(google_tweets[google_tweets['sentiment'] == -1]['tweet_text'])\n",
    "\n",
    "# Generate word clouds for each sentiment\n",
    "generate_wordcloud(pos_comments, 'WordCloud for Positive Sentiment')\n",
    "generate_wordcloud(neu_comments, 'WordCloud for Neutral Sentiment')\n",
    "generate_wordcloud(neg_comments, 'WordCloud for Negative Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the meta features have very similar distributions in training and test set which also proves that training and test set are taken from the same sample.\n",
    "\n",
    "All of the meta features have information about target as well, but some of them are not good enough such as url_count, hashtag_count and mention_count.\n",
    "\n",
    "On the other hand, word_count, unique_word_count, stop_word_count, mean_word_length, char_count, punctuation_count have very different distributions for disaster and non-disaster tweets. Those features might be useful in models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load NLTK stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Sample DataFrames (replace with actual data loading)\n",
    "# df_train = pd.read_csv('train.csv')  # Example to load train data\n",
    "# df_test = pd.read_csv('test.csv')    # Example to load test data\n",
    "\n",
    "# ... (Your feature extraction code here) ...\n",
    "\n",
    "# Set up the visualizations\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plotting the metrics for df_train\n",
    "metrics = [\n",
    "    'word_count',\n",
    "    'unique_word_count',\n",
    "    'stop_word_count',\n",
    "    'url_count',\n",
    "    'mean_word_length',\n",
    "    'char_count',\n",
    "    'punctuation_count',\n",
    "    'hashtag_count',\n",
    "    'mention_count'\n",
    "]\n",
    "\n",
    "# Create subplots\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.histplot(df_train[metric], bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {metric}')\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plotting summary statistics\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_train[metrics])\n",
    "plt.title('Boxplot of Tweet Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
